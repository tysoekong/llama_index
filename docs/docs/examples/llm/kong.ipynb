{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kong\n",
    "\n",
    "[Kong Gateway](https://konghq.com/) is a lightweight, fast, and flexible API gateway that can be\n",
    "extended through huge ecosystem of [plugins and integrations](https://docs.konghq.com/hub/).\n",
    "\n",
    "Kong is trusted for billions of transactions a day, by 700+ customers of all sizes across all industries.\n",
    "\n",
    "## Kong AI Gateway\n",
    "\n",
    "[Kong AI Gateway](https://konghq.com/products/kong-ai-gateway) delivers a suite of AI-specific plugins\n",
    "on top of the API Gateway platform, enabling you to:\n",
    "\n",
    "* Route a single consumer interface to multiple models, across many providers\n",
    "* Load balance similar models based on cost, latency, and other metrics/algorithms\n",
    "* Deliver a rich analytics and auditing suite for your deployments\n",
    "* Enable semantic features to protect your users, your models, and your costs\n",
    "* Provide no-code AI enhancements to your existing REST APIs\n",
    "* Leverage Kong's existing ecosystem of authentication, monitoring, and traffic-control plugins\n",
    "\n",
    "## Get Started\n",
    "\n",
    "Kong AI Gateway exchanges inference requests in the OpenAI formats - thus you can easily and quickly\n",
    "connect your existing LlamaIndex OpenAI adaptor-based integrations directly through Kong with no code changes.\n",
    "\n",
    "You can target hundreds of models across the [supported providers](https://docs.konghq.com/hub/kong-inc/ai-proxy/),\n",
    "all from the same client-side codebase.\n",
    "\n",
    "### Create LLM Configuration\n",
    "\n",
    "Kong AI Gateway uses the same familiar service/route/plugin system as the API Gateway product,\n",
    "with a declarative setup that launches a complete gateway system configured from a single\n",
    "YAML file.\n",
    "\n",
    "Create your gateway YAML file, using the [Kong AI-Proxy Plugin](https://docs.konghq.com/hub/kong-inc/ai-proxy/),\n",
    "in this example for:\n",
    "\n",
    "* **OpenAI** backend and **GPT-4o** model\n",
    "\n",
    "### Adjust Kong to Support LlamaIndex Authentication Headers\n",
    "\n",
    "We also add a virtual consumer, with its own API key, that can later be audited, rate-limited, etcetera.\n",
    "\n",
    "To support this in LlamaIndex, we need to configure Kong to receive the API Key header in the right location\n",
    "and format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output this file to `kong.yaml`:\n",
    "\n",
    "```yaml\n",
    "_format_version: \"3.0\"\n",
    "\n",
    "services:\n",
    "  - name: ai\n",
    "    url: https://localhost:32000\n",
    "\n",
    "    routes:\n",
    "      - name: openai-gpt4o\n",
    "        paths:\n",
    "          - \"/gpt-4o\"\n",
    "        plugins:\n",
    "          - name: ai-proxy\n",
    "            config:\n",
    "              route_type: llm/v1/chat\n",
    "              model:\n",
    "                provider: openai\n",
    "                name: gpt-4o\n",
    "              auth:\n",
    "                header_name: Authorization\n",
    "                header_value: \"Bearer <OPENAI_KEY_HERE>\"  # replace with your OpenAI key again\n",
    "\n",
    "          # Now we add a security plugin at the \"individual model\" scope\n",
    "          - name: key-auth\n",
    "            config:\n",
    "              key_names:\n",
    "                - Authorization\n",
    "\n",
    "# and finally a consumer with **its own API key**\n",
    "consumers:\n",
    "  - username: department-1\n",
    "    keyauth_credentials:\n",
    "      - key: \"Bearer department-1-api-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Gateway\n",
    "\n",
    "Launch the Kong open-source gateway, loading this configuration YAML, with one command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it --rm --name kong-ai -p 8000:8000 \\\n",
    "    -v \"$(pwd)/kong.yaml:/etc/kong/kong.yaml\" \\\n",
    "    -e \"KONG_DECLARATIVE_CONFIG=/etc/kong/kong.yaml\" \\\n",
    "    -e \"KONG_DATABASE=off\" \\\n",
    "    kong:3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Your LlamaIndex Code\n",
    "\n",
    "Now you can configure your LlamaIndex client code to point to Kong.\n",
    "\n",
    "First, load the LlamaIndex SDK into your Python dependencies, then execute the sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install llama-index-llms-openai llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run a simple chat, overriding the OpenAI URL to point to Kong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "kong_url = \"http://127.0.0.1:8000\"\n",
    "kong_route = \"gpt-4o\"\n",
    "\n",
    "llm = OpenAI(model=kong_route, api_base=f'{kong_url}/{kong_route}', api_key=\"department-1-api-key\")\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a mathematician.\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What are you?\"),\n",
    "]\n",
    "\n",
    "response = llm.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Tool Usage\n",
    "\n",
    "Kong also supports custom tools, defined via any supported OpenAI-compatible SDK, including LlamaIndex.\n",
    "\n",
    "With the same `kong.yaml` configuration, you can execute a simple custom tool definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"A result of database lookup\"\"\"\n",
    "    result: str\n",
    "    remaining: int\n",
    "\n",
    "def lookup_stock(product: str) -> Answer:\n",
    "    \"\"\"Lookup stock in product database.\"\"\"\n",
    "    return Answer(result=\"IN_STOCK\", remaining=4)\n",
    "\n",
    "tool = FunctionTool.from_defaults(fn=lookup_stock)\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "kong_url = \"http://127.0.0.1:8000\"\n",
    "kong_route = \"gpt-4o\"\n",
    "\n",
    "llm = OpenAI(model=kong_route, api_base=f'{kong_url}/{kong_route}', api_key=\"department-1-api-key\", strict=True)\n",
    "\n",
    "response = llm.predict_and_call(\n",
    "    [tool],\n",
    "    \"Is the New Phone in stock? How many are left?\",\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
